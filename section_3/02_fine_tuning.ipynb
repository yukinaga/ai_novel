{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/ai_novel/blob/main/section_3/02_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPkdg9jTjkTd"
      },
      "source": [
        "# GPT-2のファインチューニング\n",
        "青空文庫の小説を訓練データに使い、GPT-2のモデルをファインチューニングします。  \n",
        "ファインチューニングにより、モデルから漱石風の文章が生成されるようになることを確認しましょう。   \n",
        "学習に時間がかかるので、「編集」→「ノートブックの設定」の「ハードウェアアクセラレーター」で「GPU」を選択しましょう。   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPKARfZNZ_EA"
      },
      "source": [
        "## ライブラリのインストール\n",
        "GPT-2が含まれるライブラリtransformers、形態素解析（≒単語分割）のためのライブラリsentencepieceをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRTOCXhK9YAM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLSgiA6Uktpm"
      },
      "source": [
        "## GPT-2の設定\n",
        "\n",
        "今回は`rinna/japanese-gpt2-medium `をベースとして使用します。   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71dgCmmBli-s"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練データの読み込みと前処理\n",
        "以下のレポジトリをダウンロードして解凍し、section_3の「wagahaiwa_nekodearu.txt」をアップロードしましょう。  \n",
        "https://github.com/yukinaga/ai_novel  \n",
        "  \n",
        "以下のコードを実行すると、ローカル環境からファイルをアップロードできます。  "
      ],
      "metadata": {
        "id": "aBqpGzbQ7IKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # ファイルのアップロード\n",
        "file_path_original = list(uploaded.keys())[0]  # ファイルパス\n",
        "print(file_path_original)  "
      ],
      "metadata": {
        "id": "_yYT5VgfKy6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "アップロードされたファイルを読み込み、一部を表示します。  "
      ],
      "metadata": {
        "id": "iL07e0a383Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path_original, mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
        "    text_original = f.read()\n",
        "\n",
        "print(text_original[:100])  # 最初の100文字を表示"
      ],
      "metadata": {
        "id": "6vzE2VyHx8Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "正規表現を使い、ルビなどを削除します。"
      ],
      "metadata": {
        "id": "xDkWW8I89G2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = re.sub(\"《[^》]+》\", \"\", text_original)  # ルビの削除\n",
        "text = re.sub(\"［[^］]+］\", \"\", text)  # 読みの注意の削除\n",
        "text = re.sub(\"[｜ 　]\", \"\", text)  # | と全角半角スペースの削除\n",
        "print(text[:100])  # 最初の100文字を表示\n",
        "\n",
        "train_data_path = \"train.txt\"\n",
        "with open(train_data_path, mode=\"w\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "m0UAD-0Rx_HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの訓練\n",
        "既存のモデルに追加で訓練を行います。  \n",
        "まずは、訓練のための各設定を行います。  "
      ],
      "metadata": {
        "id": "I4132JNI-EAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "\n",
        "# データセットの設定\n",
        "train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_data_path,\n",
        "        block_size=128  # 文章の長さ\n",
        "        )\n",
        "\n",
        "# データの入力に関する設定\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # データをマスクするかどうか\n",
        "    )\n",
        "\n",
        "# 訓練に関する設定\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-ft\",  # 関連ファイルを保存するパス\n",
        "    overwrite_output_dir=True,  # ファイルを上書きするかどうか\n",
        "    num_train_epochs=3,  # エポック数\n",
        "    per_device_train_batch_size=8,  # バッチサイズ\n",
        "    logging_steps=100,  # 途中経過を表示する間隔\n",
        "    save_steps=800  # モデルを保存する間隔\n",
        "    )\n",
        "\n",
        "# トレーナーの設定\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "Zq9_POKQA7dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "トレーナーの`train()`メソッドにより、訓練が開始されます。 "
      ],
      "metadata": {
        "id": "MccAIyRoCi3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "g3kRQHFLIZ35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7EUJRa31Q9c"
      },
      "source": [
        "## 文章を生成する関数\n",
        "入力文章から続きの文章を生成する関数を設定します。  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getarate_sentences(seed_sentence):\n",
        "    x = tokenizer.encode(seed_sentence, return_tensors=\"pt\", add_special_tokens=False)  # 入力\n",
        "    x = x.cuda()  # GPU対応\n",
        "    y = model.generate(x, #　入力\n",
        "                       min_length=50,  # 文章の最小長\n",
        "                       max_length=100,  # 文章の最大長\n",
        "                       do_sample=True,   # 次の単語を確率で選ぶ\n",
        "                       top_k=50, # Top-Kサンプリング\n",
        "                       top_p=0.95,  # Top-pサンプリング\n",
        "                       temperature=1.2,  # 確率分布の調整\n",
        "                       num_return_sequences=3,  # 生成する文章の数\n",
        "                       pad_token_id=tokenizer.pad_token_id,  # パディングのトークンID\n",
        "                       bos_token_id=tokenizer.bos_token_id,  # テキスト先頭のトークンID\n",
        "                       eos_token_id=tokenizer.eos_token_id,  # テキスト終端のトークンID\n",
        "                       bad_word_ids=[[tokenizer.unk_token_id]]  # 生成が許可されないトークンID\n",
        "                       )  \n",
        "    generated_sentences = tokenizer.batch_decode(y, skip_special_tokens=True)  # 特殊トークンをスキップして文章に変換\n",
        "    return generated_sentences"
      ],
      "metadata": {
        "id": "epDs2MTmoAMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 文章の生成\n",
        "「吾輩は猫である」の冒頭をシードにして、ファインチューニング済みのGPT-2モデルにより小説を執筆します。"
      ],
      "metadata": {
        "id": "fHLATx0yyrcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_sentence = \"吾輩は猫である。名前はまだ無い。\"  # 吾輩は猫であるの冒頭\n",
        "generated_sentences = getarate_sentences(seed_sentence)  # 生成された文章\n",
        "for sentence in generated_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "1iN5E0gYgMHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "シードの文章にアレンジを加えましょう。"
      ],
      "metadata": {
        "id": "hzM3Ph1s7zUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_sentence = \"吾輩は犬である。名前は\"  # シード文章\n",
        "generated_sentences = getarate_sentences(seed_sentence)  # 生成された文章\n",
        "for sentence in generated_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "6czc3auU7yti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 文章の保存\n",
        "生成した文章を、txtファイルに保存します。  "
      ],
      "metadata": {
        "id": "q-qDPs4HzTaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"ft_novel.txt\", \"w\") as f:\n",
        "    f.write(generated_sentences[0])"
      ],
      "metadata": {
        "id": "gElWAtv1wl7m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMjMIdH4mWPQ4GDsPMyKJZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}